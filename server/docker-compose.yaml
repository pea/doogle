version: '3'

services:
  api:
    build:
      context: './api'
      dockerfile: 'Dockerfile'
    ports:
      - '4000:4000'
    restart: 'always'
    logging:
      driver: none
    networks:
      - network

  llamacpp:
    build:
      context: './llamaCpp'
      dockerfile: 'Dockerfile'
      args:
        MODEL_DOWNLOAD_URL: https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-Q8_0.gguf
        MMPROJ_DOWNLOAD_URL: https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-mmproj-f16.gguf

        # MODEL_DOWNLOAD_URL: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q8_0.gguf
        # MODEL_DOWNLOAD_URL: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf

        # MODEL_DOWNLOAD_URL: https://huggingface.co/cmp-nct/llava-1.6-gguf/resolve/main/ggml-mistral-q_4_k.gguf
        # MMPROJ_DOWNLOAD_URL: https://huggingface.co/cmp-nct/llava-1.6-gguf/resolve/main/mmproj-mistral7b-f16.gguf
    ports:
      - '7000:7000'
    command: '--server -m /models/llava-v1.5-7b-Q8_0.gguf --mmproj /models/llava-v1.5-7b-mmproj-f16.gguf -n 512 --n-gpu-layers 32 --port 7000 --host 0.0.0.0'
    restart: 'always'
    networks:
      - network
    logging:
      driver: none
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  whisper:
    build:
      context: './whisperCpp'
      dockerfile: 'Dockerfile'
    ports:
      - '6060:6060'
    restart: 'always'
    networks:
      - network
    logging:
      driver: none
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  tts:
    build:
      context: './tts'
      dockerfile: 'Dockerfile'
    ports:
      - '5002:5002'
    restart: 'always'
    networks:
      - network
    logging:
      driver: none
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  openwakewordtrain:
    build:
      context: './openWakeWordTraining'
      dockerfile: 'Dockerfile'
    shm_size: '3g'
    ports:
      - '8888:8888'
    volumes:
      - './openWakeWordTraining/my_custom_model/:/app/my_custom_model'
      - './openWakeWordTraining/openwakeword:/app/openwakeword'
      - './openWakeWordTraining/audioset:/app/audioset'
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  # Stable Diffusion
  #
  # stablediffusion:
  #   build:
  #     context: './stableDiffusion'
  #     dockerfile: 'Dockerfile'
  #   ports:
  #     - '3002:3002'
  #   restart: 'always'
  #  networks:
  #    - network
  #  logging:
  #    driver: none
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: 'nvidia'
  #             count: 'all'
  #             capabilities:
  #               - 'gpu'

  # Tortoise TTS Server
  # Can replace tts service but requires port change and api request changes in api service
  #
  # tortoise:
  #   build:
  #     context: './tortoise'
  #     dockerfile: 'Dockerfile'
  #   ports:
  #     - '6700:6700'
  #   volumes:
  #     - './.cache:/root/.cache'
  #  networks:
  #    - network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: 'nvidia'
  #             count: 'all'
  #             capabilities:
  #               - 'gpu'

networks:
  network: