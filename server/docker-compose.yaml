version: '3'

services:
  api:
    build:
      context: './api'
      dockerfile: 'Dockerfile'
    ports:
      - '4000:4000'
    restart: 'always'
    logging:
      driver: none
    networks:
      - network  

  llamacpp:
    image: llamacpp
    build:
      context: './llamaCppServer'
      dockerfile: 'Dockerfile'
      args:
        MODEL_DOWNLOAD_URL: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
    ports:
      - '7000:7000'
    restart: 'always'
    networks:
      - network
    logging:
      driver: none
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu' 

  mobilevlm:
    build:
      context: './mobilevlmServer'
      dockerfile: 'Dockerfile'
      args:
        MODEL_DOWNLOAD_URL: https://huggingface.co/guinmoon/MobileVLM-1.7B-GGUF/resolve/main/MobileVLM-1.7B-Q4_K.gguf
        MMPROJ_DOWNLOAD_URL: https://huggingface.co/guinmoon/MobileVLM-1.7B-GGUF/resolve/main/MobileVLM-1.7B-mmproj-f16.gguf
    ports:
      - '7002:7000'
    networks:
      - network
    logging:
      driver: none
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'    

  whisper:
    build:
      context: './whisperCpp'
      dockerfile: 'Dockerfile'
    ports:
      - '6060:6060'
    restart: 'always'
    networks:
      - network
    logging:
      driver: none
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  tts:
    build:
      context: './tts'
      dockerfile: 'Dockerfile'
    ports:
      - '5002:5002'
    restart: 'always'
    networks:
      - network
    logging:
      driver: none
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  openwakewordtrain:
    build:
      context: './openWakeWordTraining'
      dockerfile: 'Dockerfile'
    shm_size: '3g'
    ports:
      - '8888:8888'
    volumes:
      - './openWakeWordTraining/my_custom_model/:/app/my_custom_model'
      - './openWakeWordTraining/openwakeword:/app/openwakeword'
      - './openWakeWordTraining/audioset:/app/audioset'
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  # Stable Diffusion
  #
  # stablediffusion:
  #   build:
  #     context: './stableDiffusion'
  #     dockerfile: 'Dockerfile'
  #   ports:
  #     - '3002:3002'
  #   restart: 'always'
  #  networks:
  #    - network
  #  logging:
  #    driver: none
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: 'nvidia'
  #             count: 'all'
  #             capabilities:
  #               - 'gpu'

  # Tortoise TTS Server
  # Can replace tts service but requires port change and api request changes in api service
  #
  # tortoise:
  #   build:
  #     context: './tortoise'
  #     dockerfile: 'Dockerfile'
  #   ports:
  #     - '6700:6700'
  #   volumes:
  #     - './.cache:/root/.cache'
  #  networks:
  #    - network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: 'nvidia'
  #             count: 'all'
  #             capabilities:
  #               - 'gpu'

networks:
  network: