services:
  api:
    build:
      context: '.'
      dockerfile: 'api.Dockerfile'
    ports:
      - '4000:4000'
    restart: 'always'

  llamacpp:
    build:
      context: '.'
      dockerfile: 'llamaCpp.Dockerfile'
    ports:
      - '7000:7000'
    command: '--server -m /models/wizardlm-1.0-uncensored-llama2-13b.Q4_K_M.gguf -n 512 --n-gpu-layers 32 --port 7000 --host 0.0.0.0'
    restart: 'always'
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  whisper:
    build:
      context: '.'
      dockerfile: 'whisperCpp.Dockerfile'
    ports:
      - '6060:8080'
    restart: 'always'
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  tts:
    build:
      context: '.'
      dockerfile: 'tss.Dockerfile'
    ports:
      - '5002:5002'
    restart: 'always'
    deploy:
      resources:
        reservations:
          devices:
            - driver: 'nvidia'
              count: 'all'
              capabilities:
                - 'gpu'

  # stablediffusion:
  #   build:
  #     context: '.'
  #     dockerfile: 'stableDiffusion.Dockerfile'
  #   ports:
  #     - '3002:3002'
  #   restart: 'always'
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: 'nvidia'
  #             count: 'all'
  #             capabilities:
  #               - 'gpu'
